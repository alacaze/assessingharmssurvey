

,-------------------.
|  BIBTEX ENTRIES   |
`-------------------'

@article{Bresalier:2005pi,
  author =        {Bresalier, Robert S and Sandler, Robert S and
                   Quan, Hui and Bolognese, James A and Oxenius, Bettina and
                   Horgan, Kevin and Lines, Christopher and
                   Riddell, Robert and Morton, Dion and Lanas, Angel and
                   Konstam, Marvin A and Baron, John A},
  journal =       {New England Journal of Medicine},
  number =        {11},
  pages =         {1092--1102},
  title =         {{Cardiovascular events associated with Rofecoxib in a
                   colorectal andenoma chemoprevention trial}},
  volume =        {352},
  year =          {2005},
}

@article{Cumming2001,
  author =        {Cumming, Geoff and Finch, Sue},
  journal =       {Educational and Psychological Measurement},
  number =        {4},
  pages =         {532--574},
  title =         {{A Primer on the Understanding, Use, and Calculation
                   of Confidence Intervals that are Based on Central and
                   Noncentral Distributions}},
  volume =        {61},
  year =          {2001},
  abstract =      {Reform of statistical practice in the social and
                   behavioral sciences requires wider use of confidence
                   intervals (CIs), effect size measures, and
                   meta-analysis. The authors discuss four reasons for
                   promoting use of CIs: They (a) are readily
                   interpretable, (b) are linked to familiar statistical
                   significance tests, (c) can encourage meta-analytic
                   thinking, and (d) give information about precision.
                   The authors discuss calculation of CIs for a basic
                   standardized effect size measure, Cohen's $\delta$
                   (also known as Cohen's d), and contrast these with
                   the familiar CIs for original score means. CIs for
                   $\delta$ require use of noncentral t distributions,
                   which the authors apply also to statistical power and
                   simple meta-analysis of standardized effect sizes.
                   They provide the ESCI graphical software, which runs
                   under Microsoft Excel, to illustrate the discussion.
                   Wider use of CIs for $\delta$ and other effect size
                   measures should help promote highly desirable reform
                   of statistical practice in the social sciences.},
  doi =           {10.1177/0013164401614002},
}

@article{Doll2005,
  author =        {Doll, H},
  journal =       {Evidence-Based Medicine},
  number =        {5},
  pages =         {133--134},
  title =         {{Statistical approaches to uncertainty: p values and
                   confidence intervals unpacked}},
  volume =        {10},
  year =          {2005},
  doi =           {10.1136/ebm.10.5.133},
  issn =          {1356-5524},
}

@article{Falk1995,
  address =       {Thousand Oaks},
  author =        {Falk, Ruma and Greenbaum, Charles W},
  journal =       {Theory {\&} psychology},
  number =        {1},
  pages =         {75--98},
  publisher =     {Sage Publications},
  title =         {{Significance Tests Die Hard: The Amazing Persistence
                   of a Probabilistic Misconception}},
  volume =        {5},
  year =          {1995},
  abstract =      {We present a critique showing the flawed logical
                   structure of statistical significance tests. We then
                   attempt to analyze why, in spite of this faulty
                   reasoning, the use of significance tests persists. We
                   identify the illusion of probabilistic proof by
                   contradiction as a central stumbling block, because
                   it is based on a misleading generalization of
                   reasoning from logic to inference under uncertainty.
                   We present new data from a student sample and
                   examples from the psychological literature showing
                   the strength and prevalence of this illusion. We
                   identify some intrinsic cognitive mechanisms
                   (similarity to modus tollens reasoning; verbal
                   ambiguity in describing the meaning of significance
                   tests; and the need to rule out chance findings) and
                   extrinsic social pressures which help to maintain the
                   illusion. We conclude by mentioning some alternative
                   methods for presenting and analyzing psychological
                   data, none of which can be considered the ultimate
                   method.},
  issn =          {0959-3543},
}

@article{Fidler2009,
  address =       {Loftus},
  author =        {Fidler, Fiona and Loftus, Geoffrey R},
  journal =       {Zeitschrift f{\"{u}}r Psychologie/Journal of
                   Psychology},
  number =        {1},
  pages =         {27--37},
  publisher =     {Hogrefe {\&} Huber Publishers},
  title =         {{Why figures with error bars should replace p values:
                   Some conceptual arguments and empirical
                   demonstrations.}},
  volume =        {217},
  year =          {2009},
  abstract =      {Null-hypothesis significance testing (NHST) is the
                   primary means by which data are analyzed and
                   conclusions made, particularly in the social
                   sciences, but in other sciences as well (notably
                   ecology and economics). Despite this supremacy
                   however, numerous problems exist with NHST as a means
                   of interpreting and understanding data. These
                   problems have been articulated by various observers
                   over the years, but are being taken seriously by
                   researchers only slowly, if at all, as evidenced by
                   the continuing emphasis on NHST in statistics
                   classes, statistics textbooks, editorial policies
                   and, of course, the day-to-day practices reported in
                   empirical articles themselves (Cumming et al., 2007).
                   Over the past several decades, observers have
                   suggested a simpler approach? plotting the data with
                   appropriate confidence intervals (CIs) around
                   relevant sample statistics? to supplement or take the
                   place of hypothesis testing. This article addresses
                   these issues. (PsycINFO Database Record (c) 2016 APA,
                   all rights reserved)},
  doi =           {10.1027/0044-3409.217.1.27},
  issn =          {0044-3409(Print)},
}

@article{Fitchett2018,
  author =        {Fitchett, David and Butler, Javed and
                   {Van De Borne}, Philippe and Zinman, Bernard and
                   Lachin, John M. and Wanner, Christoph and
                   Woerle, Hans J. and Hantel, Stefan and
                   George, Jyothis T. and Johansen, Odd Erik and
                   Inzucchi, Silvio E.},
  journal =       {European Heart Journal},
  number =        {5},
  pages =         {363--370},
  title =         {{Effects of empagliflozin on risk for cardiovascular
                   death and heart failure hospitalization across the
                   spectrum of heart failure risk in the EMPA-REG
                   OUTCOMEVR trial}},
  volume =        {39},
  year =          {2018},
  abstract =      {{\textcopyright} The Author 2017. Aims: Empagliflozin
                   reduced the risk of cardiovascular (CV) death and
                   heart failure (HF) hospitalizations in patients with
                   type 2 diabetes (T2D) and established CV disease
                   (CVD) in the EMPA-REG OUTCOME{\textregistered} trial.
                   We investigated whether the benefit of empagliflozin
                   was observed across the spectrum of HF risk. Methods
                   and results: Seven thousand and twenty patients with
                   T2D (HbA1c 7-10{\%} and eGFR{\textgreater}
                   30mL/min/1.73 m2) were treated with empagliflozin 10
                   or 25 mg, or placebo once daily and followed for
                   median 3.1 years. In patients without HF at baseline
                   (89.9{\%}), we derived the 5-year risk for incident
                   HF using the 9-variable Health ABC HF Risk score
                   [classified as low-to-average ({\textless}10{\%}),
                   high (10-20{\%}), and very high (≥20{\%})].
                   Overall, 67.2{\%} of the population had
                   low-to-average, 24.2{\%} high, and 5.1{\%} very high
                   5-year HF risk. Across these groups, the effect on CV
                   death and HF hospitalization with empagliflozin was
                   consistent [hazard ratio 0.71 (95{\%} confidence
                   interval: 0.52, 0.96), 0.52 (0.36, 0.75), and 0.55
                   (0.30, 1.00), respectively]. Effects on CV death in
                   the ostensibly highest HF risk group (HF at baseline
                   and/or incident HF during the trial) in whom 37.9{\%}
                   of the overall CV deaths occurred, was also
                   beneficial [0.67 (0.47, 0.97)], yet, similar benefits
                   were seen in the lower risk patients. Conclusion: In
                   patients with T2D and established CVD, a sizeable
                   proportion without HF at baseline are at high or very
                   high risk for HF outcomes, indicating the need for
                   active case finding in this patient population.
                   Empagliflozin consistently improved HF outcomes both
                   in patients at low or high HF risk.},
  doi =           {10.1093/eurheartj/ehx511},
  issn =          {15229645},
}

@incollection{Gigerenzer2004,
  address =       {Thousand Oaks},
  author =        {Gigerenzer, Gerd and Krauss, Stefan and
                   Vitouch, Oliver},
  booktitle =     {The SAGE Handbook of Quantitative Methodology for the
                   Social Sciences},
  publisher =     {SAGE Publications, Inc},
  title =         {{The Null Ritual: What You Always Wanted to Know
                   About Significance Testing but Were Afraid to Ask}},
  year =          {2004},
  abstract =      {`This Handbook discusses important methodological
                   tools and topics in quantitative methodology in easy
                   to understand language. It is an exhaustive review of
                   past and recent advances in each topic combined with
                   a detailed discussion of examples and graphical
                   illustrations. It will be an essential reference for
                   social science researchers as an introduction to
                   methods and quantitative concepts of great use' -
                   Irini Moustaki, London School of Economics. `The 24
                   chapters in this Handbook span a wide range of
                   topics, presenting the latest quantitative
                   developments in scaling theory, measurement,
                   categorical data analysis, multilevel models, latent
                   variable models, and foundational issues. Each
                   chapter reviews the historical context for the topic
                   and then describes current work, including
                   illustrative examples where appropriate. The level of
                   presentation throughout the book is detailed enough
                   to convey genuine understanding without overwhelming
                   the reader with technical material. Ample references
                   are given for readers who wish to pursue topics in
                   more detail. The book will appeal to both researchers
                   who wish to update their knowledge of specific
                   quantitative methods, and students who wish to have
                   an integrated survey of state-of- the-art
                   quantitative methods' - Roger E Millsap, Arizona
                   State University. The SAGE Handbook of Quantitative
                   Methodology for the Social Sciences is the definitive
                   reference for teachers, students, and researchers of
                   quantitative methods in the social sciences, as it
                   provides a comprehensive overview of the major
                   techniques used in the field. The contributors, top
                   methodologists and researchers, have written about
                   their areas of expertise in ways that convey the
                   utility of their respective techniques, but, where
                   appropriate, they also offer a fair critique of these
                   techniques. Relevance to real-world problems in the
                   social sciences is an essential ingredient of each
                   chapter and makes this an invaluable resource. The
                   Handbook is divided into six sections: Scaling.
                   Testing and Measurement. Models for Categorical Data.
                   Models for Multilevel Data. Models for Latent
                   Variables. Foundational Issues. These sections,
                   comprising twenty-four chapters, address topics in
                   scaling and measurement, advances in statistical
                   modeling methodologies, and broad philosophical
                   themes and foundational issues that transcend many of
                   the quantitative methodologies covered in the book.
                   The Handbook is indispensable to the teaching, study,
                   and research of quantitat{\ldots}},
  isbn =          {9780761923596},
}

@article{Haller2002,
  author =        {Haller, Heiko and Krauss, Stefan},
  journal =       {Methods of psychological research MPR},
  pages =         {1--20},
  title =         {{Misinterpretations of significance: A problem
                   students share with their teachers?}},
  volume =        {7},
  year =          {2002},
  issn =          {1432-8534},
}

@article{Hemming2021,
  address =       {Australia},
  author =        {Hemming, Karla and Taljaard, Monica and Attia, John R and
                   Jones, Michael P},
  journal =       {Medical journal of Australia},
  number =        {3},
  pages =         {116----118.e1},
  title =         {{Why proper understanding of confidence intervals and
                   statistical significance is important}},
  volume =        {214},
  year =          {2021},
  issn =          {0025-729X},
}

@article{Hoekstra:2014hf,
  author =        {Hoekstra, Rink and Morey, Richard D and
                   Rouder, Jeffrey N and Wagenmakers, Eric-Jan},
  journal =       {Psychonomic Bulletin {\{}{\&}{\}} Review},
  month =         {jan},
  title =         {{Robust misinterpretation of confidence intervals}},
  year =          {2014},
  doi =           {10.3758/s13423-013-0572-3},
  url =           {http://link.springer.com/10.3758/s13423-013-0572-3},
}

@article{Lecoutre2003,
  author =        {Lecoutre, Marie‐Paule and Poitevineau, Jacques and
                   Lecoutre, Bruno},
  journal =       {International journal of psychology},
  number =        {1},
  pages =         {37--45},
  title =         {{Even statisticians are not immune to
                   misinterpretations of Null Hypothesis Significance
                   Tests}},
  volume =        {38},
  year =          {2003},
  abstract =      {We investigated the way experienced users interpret
                   Null Hypothesis Significance Testing (NHST) outcomes.
                   An empirical study was designed to compare the
                   reactions of two populations of NHST users,
                   psychological researchers and professional applied
                   statisticians, when faced with contradictory
                   situations. The subjects were presented with the
                   results of an experiment designed to test the
                   efficacy of a drug by comparing two groups
                   (treatment/placebo). Four situations were constructed
                   by combining the outcome of the t test (significant
                   vs. nonsignificant) and the observed difference
                   between the two means D (large vs. small). Two of
                   these situations appeared as conflicting (t
                   significant/D small and t nonsignificant/D large).
                   Three fundamental aspects of statistical inference
                   were investigated by means of open questions: drawing
                   inductive conclusions about the magnitude of the true
                   difference from the data in hand, making predictions
                   for future data, and making decisions about stopping
                   the experiment. The subjects were 25 statisticians
                   from pharmaceutical companies in France, subjects
                   well versed in statistics, and 20 psychological
                   researchers from various laboratories in France, all
                   with experience in processing and analyzing
                   experimental data. On the whole, statisticians and
                   psychologists reacted in a similar way and were very
                   impressed by significant results. It must be outlined
                   that professional applied statisticians were not
                   immune to misinterpretations, especially in the case
                   of nonsignificance. However, the interpretations that
                   accustomed users attach to the outcome of NHST can
                   vary from one individual to another, and it is hard
                   to conceive that there could be a consensus in the
                   face of seemingly conflicting situations. In fact,
                   beyond the superficial report of “erroneous”
                   interpretations, it can be seen in the misuses of
                   NHST intuitive judgmental “adjustments” that try
                   to overcome its inherent shortcomings. These findings
                   encourage the many recent attempts to improve the
                   habitual ways of analyzing and reporting experimental
                   data. Nous avons {\'{e}}tudi{\'{e}} la mani{\`{e}}re
                   dont des utilisateurs exp{\'{e}}riment{\'{e}}s
                   interpr{\`{e}}tent les r{\'{e}}sultats des Tests de
                   Signification de l'Hypoth{\`{e}}se Nulle. Une
                   {\'{e}}tude empirique a {\'{e}}t{\'{e}} men{\'{e}}e
                   pour comparer les r{\'{e}}actions de deux populations
                   d'utilisateurs, des chercheurs en psychologie et des
                   statisticiens professionnels, face {\`{a}} des
                   situations conflictuelles. On pr{\'{e}}sentait aux
                   sujets les r{\'{e}}sultats d'une exp{\'{e}}rience
                   planifi{\'{e}}e pour tester l'efficaci{\ldots}},
  issn =          {0020-7594},
}

@book{Oakes1986,
  address =       {Chichester ; New York},
  author =        {Oakes, Michael W},
  booktitle =     {Statistical inference : a commentary for the social
                   and behavioural sciences},
  publisher =     {Wiley},
  title =         {{Statistical inference : a commentary for the social
                   and behavioural sciences / Michael Oakes.}},
  year =          {1986},
  isbn =          {0471104434},
}

@article{Osimani2014a,
  author =        {Osimani, Barbara},
  journal =       {Preventive Medicine Reports},
  number =        {July},
  pages =         {9--13},
  title =         {{Safety vs. efficacy assessment of pharmaceuticals:
                   Epistemological rationales and methods}},
  volume =        {1},
  year =          {2014},
  abstract =      {In their comparative analysis of Randomised Clinical
                   Trials and observational studies, Papanikoloau et al.
                   (2006) assert that "it may be unfair to invoke bias
                   and confounding to discredit observational studies as
                   a source of evidence on harms". There are two kinds
                   of answers to the question why this is so. One is
                   based on metaphysical assumptions, such as the
                   problem of causal sufficiency, modularity and other
                   statistical assumptions. The other is epistemological
                   and relates to foundational issues and how they
                   determine the constraints we put on evidence. I will
                   address here the latter dimension and present recent
                   proposals to amend evidence hierarchies for the
                   purpose of safety assessment of pharmaceuticals; I
                   then relate these suggestions to a case study: the
                   recent debate on the causal association between
                   paracetamol and asthma. The upshot of this analysis
                   is that different epistemologies impose different
                   constraints on the methods we adopt to collect and
                   evaluate evidence; thus they grant "lower level"
                   evidence on distinct grounds and at different
                   conditions. Appreciating this state of affairs
                   illuminates the debate on the epistemic asymmetry
                   concerning benefits and harms and sets the basis for
                   a foundational, as opposed to heuristic,
                   justification of safety assessment based on
                   heterogeneous evidence.},
  doi =           {10.1016/j.pmedr.2014.08.002},
  issn =          {22113355},
}

@article{Phillips2019,
  author =        {Phillips, Rachel and Hazell, Lorna and Sauzet, Odile and
                   Cornelius, Victoria},
  journal =       {BMJ open},
  number =        {2},
  pages =         {e024537},
  title =         {{Analysis and reporting of adverse events in
                   randomised controlled trials: a review}},
  volume =        {9},
  year =          {2019},
  abstract =      {OBJECTIVE To ascertain contemporary approaches to the
                   collection, reporting and analysis of adverse events
                   (AEs) in randomised controlled trials (RCTs) with a
                   primary efficacy outcome. DESIGN A review of clinical
                   trials of drug interventions from four high impact
                   medical journals. DATA SOURCES Electronic contents
                   table of the BMJ, the Journal of the American Medical
                   Association (JAMA), the Lancet and the New England
                   Journal of Medicine (NEJM) were searched for reports
                   of original RCTs published between September 2015 and
                   September 2016. METHODS A prepiloted checklist was
                   used and single data extraction was performed by
                   three reviewers with independent check of a randomly
                   sampled subset to verify quality. We extracted data
                   on collection methods, assessment of severity and
                   causality, reporting criteria, analysis methods and
                   presentation of AE data. RESULTS We identified 184
                   eligible reports (BMJ n=3; JAMA n=38, Lancet n=62 and
                   NEJM n=81). Sixty-two per cent reported some form of
                   spontaneous AE collection but only 29{\%} included
                   details of specific prompts used to ascertain AE
                   data. Numbers that withdrew from the trial were well
                   reported (80{\%}), however only 35{\%} of these
                   reported whether withdrawals were due to AEs. Results
                   presented and analysis performed was predominantly on
                   'patients with at least one event' with 84{\%} of
                   studies ignoring repeated events. Despite a lack of
                   power to undertake formal hypothesis testing, 47{\%}
                   performed such tests for binary outcomes. CONCLUSIONS
                   This review highlighted that the collection,
                   reporting and analysis of AE data in clinical trials
                   is inconsistent and RCTs as a source of safety data
                   are underused. Areas to improve include reducing
                   information loss when analysing at patient level and
                   inappropriate practice of underpowered multiple
                   hypothesis testing. Implementation of standard
                   reporting practices could enable a more accurate
                   synthesis of safety data and development of guidance
                   for statistical methodology to assess causality of
                   AEs could facilitate better statistical practice.},
  doi =           {10.1136/bmjopen-2018-024537},
  issn =          {20446055},
}

@article{Vandenbroucke2008a,
  author =        {Vandenbroucke, Jan P and Psaty, Bruce M},
  journal =       {Journal of the American Medical Association},
  number =        {20},
  pages =         {2417--2419},
  title =         {{Benefits and Risks of Drug Treatments}},
  volume =        {300},
  year =          {2008},
}

@article{Wulff1987,
  address =       {England},
  author =        {Wulff, H R and Andersen, B and Brandenhoff, P and
                   Guttler, F},
  journal =       {Statistics in medicine},
  number =        {1},
  pages =         {3--10},
  title =         {{What do doctors know about statistics?}},
  volume =        {6},
  year =          {1987},
  abstract =      {A multiple choice test with nine statistical
                   questions was sent to a random sample of Danish
                   doctors to assess their knowledge of elementary
                   statistical expressions (SD, SE, p less than 0.05, p
                   greater than 0.05 and r). One hundred and forty eight
                   (59 per cent) of 250 doctors answered the questions.
                   The test was also completed by 97 participants in
                   postgraduate courses in research methods, mainly
                   junior hospital doctors. The median number of correct
                   answers was 2.4 in the random sample and 4.0 in the
                   other sample of doctors. It is concluded that the
                   statistical knowledge of most doctors is so limited
                   that they cannot be expected to draw the right
                   conclusions from those statistical analyses which are
                   found in papers in medical journals. Sixty-five per
                   cent of the doctors in the random sample stated that
                   it is very important that this problem is raised.},
  doi =           {10.1002/sim.4780060103},
  issn =          {0277-6715 (Print)},
  language =      {eng},
}

